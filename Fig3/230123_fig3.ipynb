{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n",
      "b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'exp_blip_models' from '/home/camp/warnert/working/Recordings/binary_pulses/blip_analysis/exp_blip_models.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import poisson, norm\n",
    "from copy import deepcopy\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import pickle\n",
    "import blip_analysis as ba\n",
    "import exp_blip_models as em\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import daskify\n",
    "import dask\n",
    "from scipy.stats import skewnorm, mannwhitneyu\n",
    "import scipy\n",
    "import matplotlib\n",
    "from sklearn.manifold import LocallyLinearEmbedding, MDS\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "from scipy.spatial import distance_matrix\n",
    "import openephys as oe\n",
    "from scipy.stats import ttest_ind\n",
    "import importlib\n",
    "importlib.reload(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found odour 1\n",
      "Found odour 3\n",
      "Found odour 5\n"
     ]
    }
   ],
   "source": [
    "unit_usrt1, unit_usrt2, unit_usrt3 = ba.load_usrts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glyphs = ba.get_glyphs()\n",
    "good_indexes = ba.get_stable_resp_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = em.ExponentialCustomTrialArray(unit_usrt1, good_indexes[0], em.trial_arrays['diff_array'])\n",
    "model.fit_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [03:48<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "models1 = []\n",
    "for i in trange(130):\n",
    "    model = em.ExponentialCustomTrialArray(unit_usrt1, good_indexes[i], em.trial_arrays['diff_array'])\n",
    "    model.fit_split()\n",
    "    models1.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [07:46<00:00,  3.59s/it]\n"
     ]
    }
   ],
   "source": [
    "models2 = []\n",
    "models3 = []\n",
    "for i in trange(130):\n",
    "    model = em.ExponentialCustomTrialArray(unit_usrt2, good_indexes[i], em.trial_arrays['diff_array'])\n",
    "    model.fit_split()\n",
    "    models2.append(model)\n",
    "    \n",
    "    model = em.ExponentialCustomTrialArray(unit_usrt3, good_indexes[i], em.trial_arrays['diff_array'])\n",
    "    model.fit_split()\n",
    "    models3.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models1, open('230124_full_models1.pkl', 'wb'),protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(models2, open('230124_full_models2.pkl', 'wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(models3, open('230124_full_models3.pkl', 'wb'),protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/camp/home/warnert/.conda/envs/blip_manu/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 35386 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    }
   ],
   "source": [
    "dasked = daskify.Daskified()\n",
    "dasked.start_cluster()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dasked.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dask_fit_split(model_name, model_index):\n",
    "    models = pickle.Unpickler(open(model_name, 'rb')).load()\n",
    "    model = models[model_index]\n",
    "    model.fit_split(train_test_var=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_models1 = np.array(pickle.Unpickler(open('221209_full_models1.pkl', 'rb')).load())[good_indexes]\n",
    "full_models2 = np.array(pickle.Unpickler(open('221209_full_models2.pkl', 'rb')).load())[good_indexes]\n",
    "full_models3 = np.array(pickle.Unpickler(open('221209_full_models3.pkl', 'rb')).load())[good_indexes]\n",
    "\n",
    "cao_models1 = np.array(pickle.Unpickler(open('221209_cao_models1.pkl', 'rb')).load())[good_indexes]\n",
    "cao_models2 = np.array(pickle.Unpickler(open('221209_cao_models2.pkl', 'rb')).load())[good_indexes]\n",
    "cao_models3 = np.array(pickle.Unpickler(open('221209_cao_models3.pkl', 'rb')).load())[good_indexes]\n",
    "\n",
    "conc_models1 = np.array(pickle.Unpickler(open('221209_conc_models1.pkl', 'rb')).load())[good_indexes]\n",
    "conc_models2 = np.array(pickle.Unpickler(open('221209_conc_models2.pkl', 'rb')).load())[good_indexes]\n",
    "conc_models3 = np.array(pickle.Unpickler(open('221209_conc_models3.pkl', 'rb')).load())[good_indexes]\n",
    "\n",
    "onset_models1 = np.array(pickle.Unpickler(open('221209_onset_models1.pkl', 'rb')).load())[good_indexes]\n",
    "onset_models2 = np.array(pickle.Unpickler(open('221209_onset_models2.pkl', 'rb')).load())[good_indexes]\n",
    "onset_models3 = np.array(pickle.Unpickler(open('221209_onset_models3.pkl', 'rb')).load())[good_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_models1[0])\n",
    "print(onset_models1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_futures1 = [dask.delayed(dask_fit_split)('230123_onset_models1.pkl', ui) for ui in range(130)]\n",
    "dask_futures2 = [dask.delayed(dask_fit_split)('230123_onset_models2.pkl', ui) for ui in range(130)]\n",
    "dask_futures3 = [dask.delayed(dask_fit_split)('230123_onset_models3.pkl', ui) for ui in range(130)]\n",
    "dask_futures = dask_futures1 + dask_futures2 + dask_futures3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'types.ExponentialCustomTrialArray'>: attribute lookup ExponentialCustomTrialArray on types failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d64d327e8942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# pickle.dump(list(conc_models3), open('230123_conc_models3.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset_models1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'230123_onset_models1.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset_models2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'230123_onset_models2.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monset_models3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'230123_onset_models3.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'types.ExponentialCustomTrialArray'>: attribute lookup ExponentialCustomTrialArray on types failed"
     ]
    }
   ],
   "source": [
    "# pickle.dump(list(full_models1), open('230123_full_models1.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(full_models2), open('230123_full_models2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(full_models3), open('230123_full_models3.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# pickle.dump(list(cao_models1), open('230123_cao_models1.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(cao_models2), open('230123_cao_models2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(cao_models3), open('230123_cao_models3.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# pickle.dump(list(conc_models1), open('230123_conc_models1.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(conc_models2), open('230123_conc_models2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(list(conc_models3), open('230123_conc_models3.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "pickle.dump(onset_models1, open('230123_onset_models1.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(list(onset_models2), open('230123_onset_models2.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(list(onset_models3), open('230123_onset_models3.pkl', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = em.ExponentialCustomTrialArray(unit_usrt1, 0, em.trial_arrays['diff_array'])\n",
    "onset_model_dir = dir(onset_models1[0])\n",
    "new_model_dir = np.array(dir(new_model))\n",
    "for i in range(130):\n",
    "    onset_model_dir = np.array(dir(onset_models1[i]))\n",
    "    print(onset_models1[i])\n",
    "    for j in new_model_dir:\n",
    "        if j not in onset_model_dir:\n",
    "            print(j)\n",
    "            print(setattr(onset_models1[i], j, None))\n",
    "            setattr(onset_models2[i], j, None)\n",
    "            setattr(onset_models3[i], j, None)\n",
    "print(len(onset_model_dir))\n",
    "print(len(dir(onset_models1[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "print(len(dir(new_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_models1 = []\n",
    "onset_models2 = []\n",
    "onset_models3 = []\n",
    "onset_array = em.trial_arrays['cao_array'][:, 5:]\n",
    "for i in range(130):\n",
    "    onset_models1.append(em.ExponentialCustomTrialArray(unit_usrt1, good_indexes[i], onset_array))\n",
    "    onset_models2.append(em.ExponentialCustomTrialArray(unit_usrt2, good_indexes[i], onset_array))\n",
    "    onset_models3.append(em.ExponentialCustomTrialArray(unit_usrt3, good_indexes[i], onset_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in onset_model_dir:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_models1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_outs = client.compute(dask_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 130\n",
      "finished 260\n"
     ]
    }
   ],
   "source": [
    "dasked.current_futures = dask_outs\n",
    "dasked.check_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(em)\n",
    "\n",
    "model = em.ExponentialCustomTrialArray(unit_usrt1,good_indexes[5], em.trial_arrays['diff_array'])\n",
    "model.fit_split(train_test_var=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.test_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean((model.pred_test_avg.mean(axis=0)-model.X_test_avg.mean(axis=0))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(np.mean(model.X_test_avg) - model.X_test_avg.mean(axis=0))**2)\n",
    "print(np.mean((model.pred_test_avg.mean(axis=0) - model.X_test_avg.mean(axis=0))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_models1[20].test_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean((model.pred_test_avg.mean(axis=0) - model.X_test_avg.mean(axis=0))**2)/model.X_test_avg.mean(axis=0).std()**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b12ddb180c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdask_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-b12ddb180c92>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdask_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/camp/home/warnert/.conda/envs/blip_manu/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cancelled\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-814d2d810f33>\u001b[0m in \u001b[0;36mdask_fit_split\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdask_fit_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_var\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "res = [i.result() for i in dask_outs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5a2ba4b5262b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_models1 = res[:130]\n",
    "onset_models2 = res[130:260]\n",
    "onset_models3 = res[260:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res).reshape(3, 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cao_models1, cao_models2, cao_models3 = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_models1, conc_models2, conc_models3 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_models1, onset_models2, onset_models3 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_scores1 = []\n",
    "new_scores1 = []\n",
    "init_scores2 = []\n",
    "new_scores2 = []\n",
    "init_scores3 = []\n",
    "new_scores3 = []\n",
    "for i in trange(130):\n",
    "    model = full_models1[i]\n",
    "    init_scores1.append(model.test_scores.mean())\n",
    "    new_scores1.append(res[0][i].test_scores.mean())\n",
    "    model = full_models2[i]\n",
    "    init_scores2.append(model.test_scores.mean())\n",
    "    new_scores2.append(res[1][i].test_scores.mean())\n",
    "    model = full_models3[i]\n",
    "    init_scores3.append(model.test_scores.mean())\n",
    "    new_scores3.append(res[2][i].test_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pred_test_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = full_models1[0]\n",
    "print(model.train_scores.mean())\n",
    "print(init_scores1[0])\n",
    "print(model.unit_sr_var.mean())\n",
    "print(model.X_test_avg.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(new_scores1), np.mean(init_scores1))\n",
    "print(np.mean(new_scores2), np.mean(init_scores2))\n",
    "print(np.mean(new_scores3), np.mean(init_scores3))\n",
    "print(ttest_ind(new_scores1, init_scores1))\n",
    "print(ttest_ind(new_scores2, init_scores2))\n",
    "print(ttest_ind(new_scores3, init_scores3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.violinplot([init_scores1, new_scores1, init_scores2, new_scores2, init_scores3, new_scores3])\n",
    "plt.xticks(range(1, 7), labels=['Old fit 1', 'New fit 1', 'Old fit 2', 'New fit 2', 'Old fit 3', 'New fit 3']);\n",
    "plt.ylabel('Normalised fit error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.where(np.array(new_scores1) <= 1.0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(new_scores1))\n",
    "print(np.argmax(new_scores2))\n",
    "print(np.argmax(new_scores3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(new_scores1))\n",
    "plt.plot(res[0, 5].pred_test_avg.mean(axis=0))\n",
    "plt.plot(res[0, 5].X_test_avg.mean(axis=0))\n",
    "print(np.max(new_scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(new_scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(new_scores1))\n",
    "print(np.max(new_scores1))\n",
    "print(np.where(new_scores1 == np.quantile(new_scores1, 0.5, interpolation='nearest')))\n",
    "scores_test = []\n",
    "var_test = []\n",
    "unnormed_scores = []\n",
    "for i in range(100):\n",
    "    scores_test.append(np.mean((new_model.X_test_avg[i] - new_model.pred_test_avg[i])**2)/(np.std(new_model.pred_test_avg[i])**2))\n",
    "    var_test.append(np.std(new_model.pred_test_avg[i])**2)\n",
    "    unnormed_scores.append(np.mean((new_model.X_test_avg[i] - new_model.pred_test_avg[i])**2))\n",
    "print(np.mean(scores_test))\n",
    "print(np.mean((new_model.X_test_avg.mean(axis=0) - new_model.pred_test_avg.mean(axis=0))**2)/(np.std(new_model.pred_test_avg.mean(axis=0))**2))\n",
    "plt.plot(unnormed_scores)\n",
    "plt.axhline(np.mean(unnormed_scores))\n",
    "plt.axhline(np.mean((new_model.X_test_avg.mean(axis=0) - new_model.pred_test_avg.mean(axis=0))**2))\n",
    "plt.figure()\n",
    "plt.axhline(np.mean(var_test))\n",
    "plt.plot(var_test)\n",
    "plt.axhline((np.std(new_model.pred_test_avg.mean(axis=0))**2), color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = res[0][3]\n",
    "plt.plot(new_model.pred_test_avg.mean(axis=0))\n",
    "plt.plot(new_model.X_test_avg.mean(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = res[0][2]\n",
    "print(new_model.test_scores.mean())\n",
    "print(full_models1[2].test_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(init_scores1), np.mean(new_scores1))\n",
    "print(ttest_ind(init_scores1, new_scores1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_models1, full_models2, full_models3 = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_joint_models = np.concatenate([full_models1, full_models2, full_models3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.array([np.mean(i.pred_test_avg, axis=0) for i in full_joint_models])\n",
    "test_true = np.array([np.mean(i.X_test_avg, axis=0) for i in full_joint_models])\n",
    "all_test_preds = np.array([i.pred_test_avg for i in full_joint_models])\n",
    "all_test_true = np.array([i.X_test_avg for i in full_joint_models])\n",
    "all_scores = np.array([np.mean(i.test_scores) for i in full_joint_models])\n",
    "scores1 = np.array([np.mean(i.test_scores) for i in full_models1])\n",
    "scores2 = np.array([np.mean(i.test_scores) for i in full_models2])\n",
    "scores3 = np.array([np.mean(i.test_scores) for i in full_models3])\n",
    "print(all_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = res[0][0]\n",
    "model.avg_testing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distributions(ax, errors, log=True, yticklabels=None, axvlines=None):\n",
    "    if log:\n",
    "        ax.violinplot([np.log10(i) for i in errors], vert=False)\n",
    "        ax.set_xlabel('Log(Fit error)')\n",
    "    else:\n",
    "        ax.violinplot(errors, vert=False)\n",
    "        ax.set_xlabel('Fit error')\n",
    "    ax.set_yticks(np.arange(1, len(errors)+1))\n",
    "    if yticklabels is not None:\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "    if axvlines is not None:\n",
    "        for axvline in axvlines:\n",
    "            ax.axvline(axvline, linestyle='--', color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_fit = np.where(all_scores == np.max(all_scores))[0][0]\n",
    "percentile25 = np.where(all_scores == np.percentile(all_scores, 25, interpolation='nearest'))[0][0]\n",
    "median = np.where(all_scores == np.percentile(all_scores, 50,interpolation='nearest'))[0][0]\n",
    "percentile75 = np.where(all_scores == np.percentile(all_scores, 75, interpolation='nearest'))[0][0]\n",
    "best_fit = np.where(all_scores == np.min(all_scores))[0][0]\n",
    "best_fit = np.argsort(all_scores)[0]\n",
    "fit_indexes = [best_fit, percentile25, median, percentile75, worse_fit]\n",
    "fit_index_values = [np.log10(all_scores[i]) for i in fit_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scores = [scores1, scores2, scores3]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plot_error_distributions(ax, combined_scores, yticklabels=['Odour 1', 'Odour 2', 'Odour 3'], axvlines=fit_index_values[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fits(fig, gs, true_vals, pred_vals, chosen_indexes, hor=True, legend_ax_index=2):\n",
    "    if hor:\n",
    "        subgs = gs.subgridspec(ncols=len(chosen_indexes), nrows=1)\n",
    "    else:\n",
    "        subgs = gs.subgridspec(ncols=1, nrows=len(chosen_indexes))\n",
    "    axs = []\n",
    "    for ax_i, index in enumerate(chosen_indexes):\n",
    "        if ax_i != 0:\n",
    "            \n",
    "            ax = fig.add_subplot(subgs[ax_i], sharey=axs[-1])\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "        else:\n",
    "            ax = fig.add_subplot(subgs[ax_i])\n",
    "            ax.set_ylabel('Firing rate (Hz)')\n",
    "        ax = plot_fit(ax, true_vals[index], label='True')\n",
    "        ax = plot_fit(ax, pred_vals[index], label='Prediction')\n",
    "        axs.append(ax)\n",
    "        ax.set_xticks(range(32))\n",
    "        ax.set_xticklabels(glyphs, fontsize=8, rotation=90)\n",
    "        if ax_i == legend_ax_index:\n",
    "            ax.legend()\n",
    "    \n",
    "    return axs\n",
    "\n",
    "def plot_fit(ax, vals, label=None):\n",
    "    mins = np.mean(vals, axis=0) - np.min(vals, axis=0)\n",
    "    maxs = np.max(vals, axis=0) - np.mean(vals, axis=0)\n",
    "    \n",
    "    ax.errorbar(range(32), np.mean(vals, axis=0), yerr=(mins, maxs), fmt='o', label=label, markersize=3, linewidth=1)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 2), dpi=150)\n",
    "gs = GridSpec(1, 1)\n",
    "plot_fits(fig, gs[0], all_test_true, all_test_preds, fit_indexes[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_trial_scores = np.concatenate([[np.mean(i.test_scores, axis=0) for i in models] for models in [full_models1, full_models2, full_models3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trial_scores(ax, trial_scores, sort_by_median=True):\n",
    "    if sort_by_median:\n",
    "        sorted_index = np.argsort(np.median(np.log10(trial_scores), axis=0))\n",
    "        sorted_scores = np.log10(trial_scores)[:, sorted_index]\n",
    "    else:\n",
    "        sorted_scores = np.log10(trial_scores)\n",
    "        sorted_index = range(32)\n",
    "    ax.boxplot(sorted_scores, showfliers=False)\n",
    "    ax.set_xticks(range(1, 33))\n",
    "    ax.set_xticklabels(np.array(glyphs)[sorted_index], rotation=90, fontsize=8)\n",
    "    ax.set_ylabel('Log(Fit error)')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plot_trial_scores(ax, combined_trial_scores, sort_by_median=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Change to a conc vs onset scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reses = np.load('../Fig2/221027_all_svm_outs.npy')\n",
    "preds1 = np.array(all_reses)[0, :, 0].reshape(500, 32)\n",
    "trues1 = np.array(all_reses)[0, :, 1].reshape(500, 32)\n",
    "preds2 = np.array(all_reses)[1, :, 0].reshape(500, 32)\n",
    "trues2 = np.array(all_reses)[1, :, 1].reshape(500, 32)\n",
    "preds3 = np.array(all_reses)[2, :, 0].reshape(500, 32)\n",
    "trues3 = np.array(all_reses)[2, :, 1].reshape(500, 32)\n",
    "cm1 = confusion_matrix(np.concatenate(trues1), np.concatenate(preds1), normalize='true', labels=range(32))\n",
    "cm2 = confusion_matrix(np.concatenate(trues2), np.concatenate(preds2), normalize='true', labels=range(32))\n",
    "cm3 = confusion_matrix(np.concatenate(trues3), np.concatenate(preds3), normalize='true', labels=range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loglog_error(ax, scores_conc, scores_onset, lims='equal', label=None):\n",
    "    ax.loglog(scores_conc, scores_onset, '.', label=label)\n",
    "#     ax.set_xlabel('Log concentration error (a.u.)')\n",
    "#     ax.set_ylabel('Log onset error (a.u.)')\n",
    "    if lims == 'equal':\n",
    "        max_lim = np.max([ax.get_ylim(), ax.get_xlim()])\n",
    "        min_lim = np.min([ax.get_ylim(), ax.get_xlim()])\n",
    "        ax.set_ylim(min_lim, max_lim)\n",
    "        ax.set_xlim(min_lim, max_lim)\n",
    "def plot_two_errors(fig, gs, all_scores1, all_scores2, all_scores3, all_scores4, lims='equal', label=None, orientation='h', wspace=0.5):\n",
    "    if orientation == 'h':\n",
    "        sub_gs = gs.subgridspec(ncols=2, nrows=1, wspace=wspace)\n",
    "    elif orientation == 'v':\n",
    "        sub_gs = gs.subgridspec(ncols=1, nrows=2, wspace=wspace)\n",
    "    error_ax1 = fig.add_subplot(sub_gs[0], aspect=1)\n",
    "    error_ax2 = fig.add_subplot(sub_gs[1], aspect=1)\n",
    "    for i in range(len(all_scores_conc)):\n",
    "        plot_loglog_error(error_ax1, all_scores1[i], all_scores2[i], label=f'Odour {i+1}')\n",
    "        plot_loglog_error(error_ax2, all_scores3[i], all_scores4[i])\n",
    "    max_lim = np.max([error_ax1.get_ylim(), error_ax2.get_xlim()])\n",
    "    min_lim = np.min([error_ax1.get_ylim(), error_ax2.get_xlim()])\n",
    "    error_ax1.set_ylim(min_lim, max_lim)\n",
    "    error_ax1.set_xlim(min_lim, max_lim)\n",
    "    error_ax2.set_ylim(min_lim, max_lim)\n",
    "    error_ax2.set_xlim(min_lim, max_lim)\n",
    "    return error_ax1, error_ax2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores_conc1 = [np.mean(i.test_scores) for i in conc_models1]\n",
    "test_scores_conc2 = [np.mean(i.test_scores) for i in conc_models2]\n",
    "test_scores_conc3 = [np.mean(i.test_scores) for i in conc_models3]\n",
    "\n",
    "test_scores_cao1 = [np.mean(i.test_scores) for i in cao_models1]\n",
    "test_scores_cao2 = [np.mean(i.test_scores) for i in cao_models2]\n",
    "test_scores_cao3 = [np.mean(i.test_scores) for i in cao_models3]\n",
    "\n",
    "test_scores_onset1 = [np.mean(i.test_scores) for i in onset_models1]\n",
    "test_scores_onset2 = [np.mean(i.test_scores) for i in onset_models2]\n",
    "test_scores_onset3 = [np.mean(i.test_scores) for i in onset_models3]\n",
    "\n",
    "test_scores1 = [np.mean(i.test_scores) for i in full_models1]\n",
    "test_scores2 = [np.mean(i.test_scores) for i in full_models2]\n",
    "test_scores3 = [np.mean(i.test_scores) for i in full_models3]\n",
    "\n",
    "all_scores_conc = [test_scores_conc1, test_scores_conc2, test_scores_conc3]\n",
    "all_scores_onset = [test_scores_onset1, test_scores_onset2, test_scores_onset3]\n",
    "all_scores_cao = [test_scores_cao1, test_scores_cao2, test_scores_cao3]\n",
    "all_scores_full = [test_scores1, test_scores2, test_scores3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "gs = GridSpec(1, 1)\n",
    "ea1, ea2 = plot_two_errors(fig, gs[0], all_scores_conc, all_scores_onset, all_scores_cao, all_scores_full, orientation='h')\n",
    "ea1.set_xlabel('Log concentration error (a.u.)')\n",
    "ea1.set_ylabel('Log onset error (a.u.)')\n",
    "ea2.set_xlabel('Log combination error (a.u.)')\n",
    "ea2.set_ylabel('Log temporal bin error (a.u.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(all_scores_cao), np.mean(all_scores_full))\n",
    "print(ttest_ind(np.array(all_scores_conc).flatten(), np.array(all_scores_onset).flatten()))\n",
    "print(ttest_ind(np.array(all_scores_cao).flatten(), np.array(all_scores_full).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_odour(fig, gs, true_vals, pred_vals, chosen_indexes, legend_ax_index=2):\n",
    "\n",
    "    subgs = gs.subgridspec(ncols=len(chosen_indexes), nrows=1)\n",
    "    axs = []\n",
    "    for ax_i, index in enumerate(chosen_indexes):\n",
    "        if ax_i != 0:\n",
    "            \n",
    "            ax = fig.add_subplot(subgs[ax_i], sharey=axs[-1])\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "        else:\n",
    "            ax = fig.add_subplot(subgs[ax_i])\n",
    "        ax.scatter(range(32), true_vals[index], label='True')\n",
    "        ax.scatter(range(32), pred_vals[index], label='Prediction')\n",
    "        axs.append(ax)\n",
    "        ax.set_xticks(range(32))\n",
    "        ax.set_xticklabels(glyphs, fontsize=8, rotation=90)\n",
    "        if ax_i == legend_ax_index:\n",
    "            ax.legend()\n",
    "    \n",
    "    return axs\n",
    "\n",
    "def pred_other_odour(model1, model2, model3, unit_usrt, ui, pred_trial = -1, ratio_schema = 'pred_trial', include_t=False):\n",
    "    full_array = em.trial_arrays['diff_array']\n",
    "    if ratio_schema == 'pred_trial':\n",
    "        ratio_w1 = model1.true_resp[-1]/((model1.true_resp[-1]+model2.true_resp[-1]))\n",
    "        ratio_w2 = model2.true_resp[-1]/((model1.true_resp[-1]+model2.true_resp[-1]))\n",
    "    elif ratio_schema == 'mean':\n",
    "        ratio_w1 = np.mean(model1.true_resp)/((np.mean(model1.true_resp)+np.mean(model2.true_resp)))\n",
    "        ratio_w2 = np.mean(model2.true_resp)/((np.mean(model1.true_resp)+np.mean(model2.true_resp)))\n",
    "    elif ratio_schema == 'max':\n",
    "        if model1.true_resp[-1] > model2.true_resp[-1]:\n",
    "            ratio_w1 = 1\n",
    "            ratio_w2 = 0\n",
    "        else:\n",
    "            ratio_w1 = 0\n",
    "            ratio_w2 = 1\n",
    "    if not include_t:\n",
    "        pred_ws = model1.opt_out.x[:-1]*ratio_w1 + model2.opt_out.x[:-1]*ratio_w2\n",
    "        avg_thresh = (model1.opt_out.x[-1] + model2.opt_out.x[-1])/2\n",
    "        pred_out = pred_ws @ full_array[pred_trial]\n",
    "        scale = (np.log(model3.true_resp[pred_trial]) - avg_thresh)/pred_out\n",
    "        pred_ws = scale*pred_ws\n",
    "        pred_ws = list(pred_ws) + [avg_thresh]\n",
    "    else:\n",
    "        pred_ws = model1.opt_out.x*ratio_w1 + model2.opt_out.x*ratio_w2\n",
    "        full_array_1 = np.append(full_array, np.ones((32, 1)), axis=1)\n",
    "        pred_out = pred_ws @ full_array_1[pred_trial]\n",
    "        scale = (np.log(model3.true_resp[pred_trial]))/pred_out\n",
    "        pred_ws = scale*pred_ws\n",
    "    model_test = em.ExponentialCustomTrialArray(unit_usrt, ui, full_array)\n",
    "    model_test.fit(W=pred_ws)\n",
    "    return model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_scores1 = []\n",
    "pred_scores2 = []\n",
    "pred_scores3 = []\n",
    "\n",
    "pred_models1 = []\n",
    "pred_models2 = []\n",
    "pred_models3 = []\n",
    "for ui in tqdm(good_indexes, position=0, leave=False):\n",
    "    model_test = pred_other_odour(full_models2[ui], full_models3[ui], full_models1[ui], unit_usrt1, ui, include_t=True)\n",
    "    pred_scores1.append(model_test.fit_score)\n",
    "    pred_models1.append(model_test)\n",
    "    model_test = pred_other_odour(full_models1[ui], full_models3[ui], full_models2[ui], unit_usrt2, ui, include_t=True)\n",
    "    pred_scores2.append(model_test.fit_score)\n",
    "    pred_models2.append(model_test)\n",
    "    model_test = pred_other_odour(full_models2[ui], full_models1[ui], full_models3[ui], unit_usrt3, ui, include_t=True)\n",
    "    pred_scores3.append(model_test.fit_score)\n",
    "    pred_models3.append(model_test)\n",
    "    \n",
    "\n",
    "pred_models_pred1 = [i.pred_resp for i in pred_models1]\n",
    "pred_models_pred2 = [i.pred_resp for i in pred_models2]\n",
    "pred_models_pred3 = [i.pred_resp for i in pred_models3]\n",
    "\n",
    "pred_models_true1 = [i.true_resp for i in pred_models1]\n",
    "pred_models_true2 = [i.true_resp for i in pred_models2]\n",
    "pred_models_true3 = [i.true_resp for i in pred_models3]\n",
    "\n",
    "all_pred_models_pred = np.concatenate([pred_models_pred1, pred_models_pred2, pred_models_pred3])\n",
    "all_pred_models_true = np.concatenate([pred_models_true1, pred_models_true2, pred_models_true3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8.3, 11.7))\n",
    "gs = GridSpec(5, 4, hspace=0.6)\n",
    "ea1, ea2 = plot_two_errors(fig, gs[0, 2:], all_scores_conc, all_scores_onset, all_scores_cao, all_scores_full, orientation='h', wspace=0.6)\n",
    "ea1.set_xlabel('Log(concentration error)')\n",
    "ea1.set_ylabel('Log(onset error)')\n",
    "ea2.set_xlabel('Log(combination error)')\n",
    "ea2.set_ylabel('Log(temporal error)')\n",
    "diagram_ax = fig.add_subplot(gs[0, :-2], frameon=False)\n",
    "diagram_ax.set_xticks([])\n",
    "diagram_ax.set_yticks([])\n",
    "plot_axes = plot_fits(fig, gs[1, :], all_test_true, all_test_preds, fit_indexes[1:-1])\n",
    "dist_ax = fig.add_subplot(gs[2, :])\n",
    "plot_error_distributions(dist_ax, combined_scores, yticklabels=['Odour 1', 'Odour 2', 'Odour 3'], axvlines=fit_index_values[1:-1])\n",
    "trial_error_ax = fig.add_subplot(gs[3, :])\n",
    "plot_trial_scores(trial_error_ax, combined_trial_scores)\n",
    "#other_odour_pred_ax = fig.add_subplot(gs[5, :])\n",
    "#plot_error_distributions(other_odour_pred_ax, [pred_scores1, pred_scores2, pred_scores3])\n",
    "#pred_odour_ax = fig.add_subplot(gs[4, :])\n",
    "#plot_pred_odour(fig, gs[4, :], all_pred_models_true, all_pred_models_pred, fit_indexes_pred_odour[1:-1])\n",
    "# heatmap_ax = fig.add_subplot(gs[1, :])\n",
    "# vmax = np.max(np.abs(firing_difference1))\n",
    "# link = linkage(np.array(firing_difference1), method='complete', optimal_ordering=True)\n",
    "# dend = dendrogram(link, no_plot=True)\n",
    "# im = heatmap_ax.imshow(np.array(firing_difference1)[dend['leaves']].T, cmap='bwr', vmax=vmax, vmin=-vmax)\n",
    "# divider = make_axes_locatable(heatmap_ax)\n",
    "# cax = divider.append_axes(\"right\", size='2%', pad=0.05)\n",
    "# plt.colorbar(im, cax=cax)\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pred odour further work\n",
    "full_responding_units = []\n",
    "for i in good_indexes:\n",
    "    model1 = full_models1[i]\n",
    "    model2 = full_models2[i]\n",
    "    model3 = full_models3[i]\n",
    "    \n",
    "    odour1_resp_diff = abs(model1.true_resp[0] - model1.true_resp[-1])\n",
    "    odour2_resp_diff = abs(model2.true_resp[0] - model2.true_resp[-1])\n",
    "    odour3_resp_diff = abs(model3.true_resp[0] - model3.true_resp[-1])\n",
    "    \n",
    "    odour1_var = np.sqrt(model1.unit_sr_var[0])\n",
    "    odour2_var = np.sqrt(model2.unit_sr_var[0])\n",
    "    odour3_var = np.sqrt(model3.unit_sr_var[0])\n",
    "    if i == 41:\n",
    "        print(odour1_resp_diff, odour1_var)\n",
    "    if odour1_resp_diff > odour1_var and odour2_resp_diff > odour2_var and odour3_resp_diff > odour3_var:\n",
    "        full_responding_units.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_responding_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_and_plot_other_odour(fig, gs, model1, model2, model3, unit_usrt, ui):\n",
    "    pred_model = pred_other_odour(model1, model2, model3, unit_usrt, ui)\n",
    "    sub_gs = gs.subgridspec(1, 3)\n",
    "    ax1 = fig.add_subplot(sub_gs[0])\n",
    "    ax2 = fig.add_subplot(sub_gs[1])\n",
    "    ax3 = fig.add_subpl\n",
    "    \n",
    "    ax1.plot(model1.true_resp)\n",
    "    ax1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12, 4))\n",
    "ui=38\n",
    "pred_model1 = pred_other_odour(full_models3[ui], full_models2[ui], full_models1[ui], unit_usrt1, ui)\n",
    "pred_model2 = pred_other_odour(full_models3[ui], full_models1[ui], full_models2[ui], unit_usrt2, ui)\n",
    "pred_model3 = pred_other_odour(full_models1[ui], full_models2[ui], full_models3[ui], unit_usrt3, ui)\n",
    "full_models1[ui].fit()\n",
    "full_models2[ui].fit()\n",
    "full_models3[ui].fit()\n",
    "\n",
    "ax[0].plot(pred_model1.true_resp, '.')\n",
    "ax[0].plot(full_models1[ui].pred_resp, '.')\n",
    "ax[0].plot(pred_model1.pred_resp, '.')\n",
    "ax[0].set_title(f'F error {full_models1[ui].fit_score:0.2f}\\nO error {pred_model1.fit_score:0.2f}')\n",
    "\n",
    "ax[1].plot(pred_model2.true_resp, '.')\n",
    "ax[1].plot(full_models2[ui].pred_resp, '.')\n",
    "ax[1].plot(pred_model2.pred_resp, '.')\n",
    "ax[1].set_title(f'F error {full_models2[ui].fit_score:0.2f}\\nO error {pred_model2.fit_score:0.2f}')\n",
    "\n",
    "\n",
    "ax[2].plot(pred_model3.true_resp, '.', label='True response')\n",
    "ax[2].plot(full_models3[ui].pred_resp, '.',label='Full predictoion')\n",
    "ax[2].plot(pred_model3.pred_resp, '.',label='Odour prediction')\n",
    "ax[2].set_title(f'F error {full_models3[ui].fit_score:0.2f}\\nO error {pred_model3.fit_score:0.2f}')\n",
    "\n",
    "ax[2].legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_pred1 = []\n",
    "scores_pred2 = []\n",
    "scores_pred3 = []\n",
    "\n",
    "scores_full1 = []\n",
    "scores_full2 = []\n",
    "scores_full3 = []\n",
    "\n",
    "\n",
    "for ui in full_responding_units:\n",
    "    pred_model1 = pred_other_odour(full_models3[ui], full_models2[ui], full_models1[ui], unit_usrt1, ui)\n",
    "    pred_model2 = pred_other_odour(full_models3[ui], full_models2[ui], full_models2[ui], unit_usrt2, ui)\n",
    "    pred_model3 = pred_other_odour(full_models1[ui], full_models2[ui], full_models3[ui], unit_usrt3, ui)\n",
    "    scores_pred1.append(pred_model1.fit_score)\n",
    "    scores_pred2.append(pred_model2.fit_score)\n",
    "    scores_pred3.append(pred_model3.fit_score)\n",
    "    full_models1[ui].fit()\n",
    "    full_models2[ui].fit()\n",
    "    full_models3[ui].fit()\n",
    "    scores_full1.append(full_models1[ui].fit_score)\n",
    "    scores_full2.append(full_models2[ui].fit_score)\n",
    "    scores_full3.append(full_models3[ui].fit_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.add_subplot(111, aspect='equal')\n",
    "plt.scatter(np.log10(scores_full1), np.log10(scores_pred1))\n",
    "plt.scatter(np.log10(scores_full2), np.log10(scores_pred2))\n",
    "plt.scatter(np.log10(scores_full3), np.log10(scores_pred3))\n",
    "plt.xlim(-2, 3.5)\n",
    "plt.ylim(-2, 3.5)\n",
    "plt.xlabel('Log(Full fit score)')\n",
    "plt.ylabel('Log(Pred from other odour fit score)')\n",
    "#plt.plot(range(-5, 5), range(-5, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([scores_full1, scores_pred1, scores_full2, scores_pred2, scores_full3, scores_pred3], showfliers=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_full1)\n",
    "print(scores_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_fits1 = np.argsort(scores_pred1)\n",
    "ranked_fits2 = np.argsort(scores_pred2)\n",
    "ranked_fits3 = np.argsort(scores_pred3)\n",
    "ranked_fits_ui1 = np.array(full_responding_units)[ranked_fits1]\n",
    "ranked_fits_ui2 = np.array(full_responding_units)[ranked_fits2]\n",
    "ranked_fits_ui3 = np.array(full_responding_units)[ranked_fits3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_three_pred_model(ui):\n",
    "    fig, ax = plt.subplots(1, 3, sharey=True, figsize=(12, 4))\n",
    "    pred_model1 = pred_other_odour(full_models3[ui], full_models2[ui], full_models1[ui], unit_usrt1, ui)\n",
    "    pred_model2 = pred_other_odour(full_models3[ui], full_models1[ui], full_models2[ui], unit_usrt2, ui)\n",
    "    pred_model3 = pred_other_odour(full_models1[ui], full_models2[ui], full_models3[ui], unit_usrt3, ui)\n",
    "    full_models1[ui].fit()\n",
    "    full_models2[ui].fit()\n",
    "    full_models3[ui].fit()\n",
    "\n",
    "    ax[0].plot(pred_model1.true_resp, '.')\n",
    "    ax[0].plot(full_models1[ui].pred_resp, '.')\n",
    "    ax[0].plot(pred_model1.pred_resp, '.')\n",
    "    ax[0].set_title(f'F error {full_models1[ui].fit_score:0.2f}\\nO error {pred_model1.fit_score:0.2f}')\n",
    "\n",
    "    ax[1].plot(pred_model2.true_resp, '.')\n",
    "    ax[1].plot(full_models2[ui].pred_resp, '.')\n",
    "    ax[1].plot(pred_model2.pred_resp, '.')\n",
    "    ax[1].set_title(f'F error {full_models2[ui].fit_score:0.2f}\\nO error {pred_model2.fit_score:0.2f}')\n",
    "\n",
    "\n",
    "    ax[2].plot(pred_model3.true_resp, '.', label='True response')\n",
    "    ax[2].plot(full_models3[ui].pred_resp, '.',label='Full predictoion')\n",
    "    ax[2].plot(pred_model3.pred_resp, '.',label='Odour prediction')\n",
    "    ax[2].set_title(f'F error {full_models3[ui].fit_score:0.2f}\\nO error {pred_model3.fit_score:0.2f}')\n",
    "\n",
    "    ax[2].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_three_pred_model(ranked_fits_ui1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranked_fits1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ffa59c630a18ee6a23737b9c50856d2054378dc4afe1e999cec46d2ba8492ecc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
